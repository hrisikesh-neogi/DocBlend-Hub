{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f88120b-02e4-490a-9968-885b7a4cee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f91d0b6-30fc-4c57-bf5d-b6262b1f41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566d807e-4915-4097-9768-598eb0cd5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "model = OpenAI(temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a21334b-551d-4b61-915a-37800bfeb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(r\"D:\\DataSquad_Tasks\\summarizer\\artifacts\\10_29_2023_01_57_36\\transcription\\transcript.txt\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5dc500e-6996-487e-8ca7-3116918d688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" It is my pleasure to welcome Dr. Andrew Wu tonight. Andrew is the Managing General Partner of AI Fund, founder of Deep Learning AI and lending AI, chairman and co-founder of Coursera, and an unjunct professor of computer science here at Stanford. Previously he had started and led the Google Brain team, which had helped Google adopt modern AI. And he was also director of the Stanford AI lab. About 8 million people, one in 1000 persons on the planet have taken an AI class from him and through both his education and his AI work. He has changed humor's lives. Please welcome Dr. An. Thank you Lisa. It's good to see everyone. So what I want to do today is chat new electricity. One of the difficult things to understand about AI is that it is a general purpose technology. Meaning that it's not useful only for one thing, but it's useful for lots of different applications. Kind of like electricity. If I were to ask you what is electricity good for? You know, it's not any one thing, there's a lot of things. So what do I like to do is start off sharing with you how I view the technology landscape and just to lead into the set of opportunities. So, a lot of hype, a lot of excitement about AI. And I think a good way to think about AI is as a collection of tools. So this includes a technique called supervised learning, which is very good at recognizing things or labeling things, and genitive AI, which is relatively new, exciting development. If you're familiar with AI, you may have heard of other tools, but I'm going to talk less about these additional tools. And I'll focus today on what I think are currently the two most important tools which are supervised learning and generative AI. So supervised learning is very good at labeling things or very good at computing input to outputs or A to B mappings given input A, give me an output B. For example, given an email, we can use supervised learning to label a spam or not spam. The most lucrative application of this that I've ever worked on is Pravee Online advertising where I give it an ad we can label the users likely to click on it and therefore show more relevant ads. For self-driving calls given the sense of readings of a car we can label it with where the other cars One project that my team at AFN worked on was shift route optimization. We're given a route to the ship is taking or considering taking. We can label that with how much fuel we think is to consume and use this chips more fuel efficient. The law work in automated visual inspection in factories. So you can take a picture of a smartphone that was just manufactured and labeled as a scratch or any other defect in it. Or if you want to build a restaurant review reputation monitoring system, you can have little piece of software that looks at online restaurant reviews and labels that as positive or negative sentiment. So one nice thing, one cool thing about supervised learning is that it's not useful for one thing. It's useful for all of these different applications and many more besides. Let me just walk through, you know, concretely, the workflow of one example of a supervised learning labeling things kind of project. If you want to build a system to label restaurant reviews, you then collect a few data points, so collect the data set where it say, you know, the best strategy sandwich great to say that this positive circles is slow. That's negative. My favorite should be curry is positive. And here I've shown three data points, but you're building this, you may get thousands of data points like this, or thousands of training examples, we call it. And the workflow of a machine learning project, when AI project is you get labeled data, maybe thousands of data points, then you have an AI entry team train an AI model to learn from this data. And then finally, you would find maybe a cloud service to run the trained I model and they can feed it, you know, less multi-either-vahad and less positive sentiment. And so I think the launch decade was maybe the decade of large scale supervised learning. What we found starting about 10, 15 years ago was if you were to train a small AI model, so train a small neural network with small deep learning algorithm, basically a small AI model, maybe not on a very powerful computer. Then as you've had more data, as performance, would get better for a little bit, but then it would flatten out, it would plateau, and it would stop being able to use the data to get better and better. But if you were to train a very large AI model, lots of compute on maybe powerful GPUs. Then as we scaled up the amount of data we gave the machine learning model, its performance would kind of keep on getting better and better. So this is why when I started and led the Google Brain team, the primary mission that I directed the team to solve at the time was, let's just build really, really large neural networks that we then fed a lot of data to and that recipe fortunately worked. And I think the idea of driving large compute and large scale data that recipes really helped us driven a lot of AI progress over the last decade. So if that was the last decade of AI, I think this decade is turning out to be also doing everything we had in supervised learning, but adding to it the exciting tool of Genese AI. So, many of you, maybe all of you, were played with charge-gbd and bar and so on. But just, you know, given a piece of text, which we call prompt. I love eating. If you run this multiple times, maybe you get bigger screen keys or my mother's me love or outer friends and the AI system can generate output like that. Given the amounts of buzz and excitement about genesis of AI, I thought I'd take just half a slide to see a little bit about how this works. So it turns out that, at least this type of text generation, the core of it is using supervised learning that inputs output mappings to repeatedly predict the next word. And so if your system reads on the internet to sentence like, my favorite food is a bagel with cream cheese and locks, then this is translated into a few data points where if it sees my favorite food is a in just case try to guess that the right next word was bagel or my favorite food is a bagel try to guess next word is worth and similarly you've a season So by taking text that you find on the internet or other sources and by using this input output supervised, to try to repeatedly predict the next word. If you train a very large AI system on hundreds of billions of words, or in the case of the largest model is now more than a trillion words, then you get a large language model like Chaggb. And, you know, there are additional other important technical details I talked about predicting the next word. Technically, these systems predict the next subword, a part of work called token, and then there are other techniques like RHSF for further tuning the air output to be more helpful on this and harmless. But at the heart of it is this using supervised learning to repeatedly predict the things where that that's really was enabling the exciting, you know, really fantastic progress on large language models. So, while many people have seen large language models as a fantastic consumer too, You can go to a website like check TV's website or bots or other launch language malls and users. I think it's fantastic too. There's one of the trends I think is still underappreciated, which is the power of large language models, not as it, not just as consumer to, but as it develop a to. So it turns out that there are applications that used to take me months to build that a lot of people can now build much faster by using a large language model. So specifically, the workflow for supervised learning, building the restaurant review system, say, would be that you need to get a bunch of labeled data, and maybe that takes a month to get a few thousand data points. And then have an AI team train and tune and really get optimized performance on your AI model, maybe that would take three months. Then find a cloud service to run it, make sure it's running robustly, make sure it's recognized, maybe that would take another three months. So pretty realistic timeline for building a commercial grade machine learning system is like six to 12 months. So teams I've led're often took roughly six or 12 months to build and deploy these systems and some of them turned out to be really valuable, but this is a realistic timeline for building and deploying a commercial-grade AI system. In contrast, with prompt base AI, where you write a prompt, this is what the workflow looks like. You can specify a prompt that takes maybe minutes or hours and then you can deploy it to the cloud and that takes maybe hours or days. So there are certain AI applications that used to take me literally six months, maybe a year to build that many teams around the world can now build in maybe a week. And I think this is already starting, but the best is still yet to come. This is starting to open up a flood of a lot more AI applications that can be built by a lot of people. So I think many people still underestimate the magnitude of the flood of customer applications that I think is going to come down the pipe. Now, I know you probably were not expecting me to write code in this presentation, but that's what I'm going to do. So it turns out this is all the code I need in order to write a sentence in classifier. So I'm gonna, you know, solve you with no Python, I guess, import some tools from OpenAI, and then I have this prompt that says classified detects the low, delimited by three dashes is having either a positive or negative sentiments. I don't know. to have a fantastic time Learned a lot and also made great new friends. So that's my problem. And now I'm just going to run it. And I've never run it before. So I really hope that thank goodness we got the right answer. And this is literally all the code it takes the build a sentiment classifier. And so today, you know, developers around the world can take literally maybe like 10 minutes to build a system like this. And that's a very exciting development. So one of the things I've been working on was trying to teach online classes about how to use prompt promoting, not just as a consumer, but as a developer too. So, just about the technology landscape. Let me now share my thoughts on what are some of the AI opportunities I see. This shows what I think is the value about three years from now. But the vast majority of financial value from AI today is I think supervised learning where for a single company like Google can be worth more than $100 billion a year. And also, there are millions of developers building supervised learning applications. So it's already massively valuable and also with tremendous momentum behind it just because of the sheer effort in finding applications and building applications. And in Genose Bay, I is the really exciting new entrance, which is much smaller right now. And then there are the other tools's I'm including for completeness we can you know If the size of these circles represent the value today, this is what I think it might grow to in three years. So supervised learning already really massive may double say in the next three years, from truly massive to even more massive. And James Sabei, which is much smaller today, I think we'll much more than double in the next three years because of the number of amounts of developer interest, the amount of venture capital investments and large corporate exploring applications. And I also just want to point out, three years is a very short time horizon. If it continues to compound at anything near this rate, then in six years,, will be even faster larger. But just light shaded region in green or orange, that light shaded region is where the opportunities for either new startups offer large companies and companies to create and create a value capture. technologies are general purpose technologies. So in the case of supervised learning, a lot of the work that had to be done over the last decade, but it's continuing for the next decade is to identify and to execute on the concrete use cases. And that process is also kicking off for Genes of AI. So for this part of the presentation, I hope you take away from it that general purpose technologies are useful for many different tasks. A lot of value remains to be created using supervised learning and even though we're nowhere near finishing figure out exciting use cases of supervised learning where there's other fantastic two of genus avi which further expands the set of things we can now do using a i But one caveat, which is that there will be short-term fads along the way. So I don't know if some of you might remember the app called LENZA. This is the app that will let you upload pictures of yourself and then render a cool picture of you as an astronaut or a scientist or something. And it was a good idea and people liked it. And it's rough, it's just so cold like crazy like that through last December. And then it did that. And that's because lensa was, it was a good idea, people liked it, but it was a relatively thin software layer on top of someone else's really powerful APIs. And so even though it was a useful product, It was in a defensive all-possess. And when I think about, you know, absolute lenser. I'm actually reminded that when Steve Jobs gave us the iPhone, shortly after someone wrote an app that I paid $1.199 for to turn on the LED, to turn the phone into flashlight. And that was also a good idea to write an app to turn on the LED light, but it was a defensible long turn. It also didn't create very long-term value because it was a easy replicated and underpriced and eventually incorporated into iOS. But with the rise of iOS, with the rise of iPhone, someone also figured out how to build things like Uber and Airbnb and Tinder. The very long term, very defensible businesses that created, you know, sustaining value. And I think with the rise of gender AI or the rise of new AI tools, I think what really excites me is the opportunity to create those really deep, really hard applications that hopefully can create very long-term value. So the first trend I want to share is as a general purpose technology and a lot of work that lies ahead of us is to find the very diverse use cases and to build them. This is a second trend I want to share with you. We'll release the YA AI isn't more widely adopted yet. It feels like a bunch of us have been talking about AI for like 15 years or something, but if you look at where the value of AI is today, a lot of it is still very concentrated and consumer-software internet. Once you go outside, you know, tech or consumer software internet, there's some air adoption but the law views very early. So why is that? It turns out if you were to take all current and potential AI projects and sought them in decreasing order value. Then to the left of this curve, the head of this curve are the multi billion dollar project the for e-commerce, product recommendations, or company Amazon. It turns out that about 10, 15 years ago, various of my friends and I, we figured out a recipe for how to hire, say, a hundred engineers to write one piece of software to serve more relevant ads and apply that one piece of software to billion users and generate massive financial value. So that works. But once you go outside, consume a software internet, how do anyone has 100 million or a billion users, they can write and apply one piece of software to you. So once you go to other industries as we go from the head of this curve on the left over to the long tail, these taking pictures of the piece they were making because they needed to do things like make sure that the cheese is spread evenly. So this is about a $5 million project, but that recipe of hiring 100 engineers or dozens of engineers to work on a five million dollar project that doesn't make sense. Or another example, working with an agriculture company that would then we figured out that we used cameras to find out how tall is the wheat. A wheat is often bento because of wind or rain or something. And we can chop off the wheat at the right height, then that results in more food for the farmer to sell, and it's also better for the environment. But this is another $5 million project that owed recipe of having a large group of high school engineers to work on this one project that doesn't make sense. And some of the materials grading, cloth grading, sheet metal grading, many project like this. So whereas to the left in the head of this curve there's a small number of let's say multi-billion dollar projects and we know how to execute those delivering value. In other industries, I'm seeing a very long tail of tens of thousands of, let's call them, five million dollar projects, then until now, it had been very difficult to excuse on because of the high cost of customization. The trend that I think is exciting is that the AI community has been building better tools that lets us aggregate these use cases and make it easy for the end user to do the customization. So specifically, I'm seeing a lot of exciting low code and no code tools that enable the user to customize the AI system. What this means is instead of me needing to worry that much about pictures of pizza. We have tools, we can start into C2s that can enable the IT department and the PISA making factory to train AI systems on their own pictures of PISA to realize this $5 million worth of value. And by the way, the pictures of Pizzer, they don't exist on the internet. So Google and Bing don't have access to these pictures. We need tools that can be used by really the piece of factory themselves to build and deploy and maintain their own custom AI system that works on their own pictures of PISA. And broadly, the technology for enabling this. Some of it is prompting. Text prompting, visual prompting, but really large language models and similar tools like that. Or a technology called data centcentric AI whereby instead of asking the piece of factory to write a lot of code, which is challenging. We can ask them to provide data, which turns out to be more feasible. And I think the second trend is important because I think this is a key part of the recipe for taking the value of AI, which so far still feels very concentrated in the tech world and consumer software into that world and pushing this out to all industries really to the rest of the economy, which sometimes is easy to forget. The rest of the economy is much bigger than the tech world. So, R2 trends are shared as a general purpose technology technology lots of compute use cases to be realized as well as local no code easily used tools enabling AI to be deployed in more industries. How do we go after these opportunities? So about five years ago, there was a puzzle I wanted to solve, which is I felt that many valuable AI projects are now possible. I was thinking, how do we get them done. And having led AI teams in Google and by do in big tech companies, I had a hard time figuring out how I could operate a team in a big tech company to go off. There are very diverse opportunities and everything from maritime shipping to education to financial services, the healthcare and all that. It's just very diverse use cases, very diverse, go to markets and very diverse, really, you know, customer bases and then applications. And I felt that the most efficient way to do this would be we can start a lot of different companies to pursue these very diverse opportunities. So that's why I end up starting AI fun, which is a venture studio that builds startups to pursue a diverse set of our opportunities. And of course, in addition to lots of startups in company companies also have a lot of opportunities to integrate AI into existing businesses. In fact, one pattern I'm seeing for incumbent businesses is distribution. distribution is often one of the cyclical advantages of incoming companies that they play the cards right can allow them to integrate So, I think of this as a, this is why I think of it as the AI stack. At the bottom level is the hardware semiconductor layer. Fantastic opportunities there but very capital intensive, very concentrated. So there's a lot of resources around the few winners. So some people can and should play there. I personally don't like to play there myself. There's also the infrastructure layer. Also fantastic opportunities, but very capital intensive, very concentrated, so I tend not to play that myself either. And then there's a developer tool later. What I showed you just now was I was actually using OpenAI's API as a developer tool. And then I think the developer tool sector is a hyper competitive. Look at all the startups chasing OpenAI right now, but there will be some mega winners. And so I sometimes play here, but primarily when I think of a meaningful technology advantage because I think that earns you the right or earns you a better shot at being one of the mega winners. And then lastly, even though a lot of the media attention in the buzz is in the infrastructure and developer tooling layer, it turns out that that layer can the application layer is even more successful. And we saw this with the rise of SAS as well. A lot of the buzz and excitement is on the technology, the tooling layer, which is fine, nothing wrong with that. But the only way for that to be successful is that the application layer is even more successful so that frankly they can generate enough revenue to pay the infrastructure and the tooling layer. So, let me mention one example. ArmorRy, it's actually just texting the CO yesterday. ArmorRy is a company that we built that uses AI for romantic relationship coaching. And just to point out, I'm an AI guy and I feel like I know nothing really about romance. And if you don't believe me, you can ask my wife if she will confirm that I know nothing about romance. But we want to build this. We want to get together with the former CEO of Tinder, a Renata Nibalb, and with my team's expertise in AI and her expertise in relationships. When she ran Tinder, she knows more about relationships. I think anyone I know, were able to build something pretty unique using AI for romantic relationship mentoring. The interesting thing about applications like these is when we look around, how many teams in the world are simultaneously expert in AI and in relationships. And so at the application layer, I'm seeing a lot of exciting opportunities that seem to the very large market, but where the competition set is very light relative to the magnitude of the opportunity. It's not that there are no competitors, but it's just much less intense compared to the developer tool of the infrastructure of the era. And so because I've spent a lot of time iterating on a process of building startups. What we're going to do is just very transparent to tell you the recipe we've developed for building startups. And so after many years of iteration and improvement, this is how we now build startups. My team's always had accesss a lot of different ideas, your internal generated ideas from partners, and I want to walk through this with one example or something we did, which is a company bearing AI, which uses AI to make ships more fuel efficient. So this idea came to me when a few years ago a large Japanese conglomerate calledui, that is a major shareholder in the opera's major shipping lines. They came to me and they said, hey, Andrew, you should build a business to use AI to make ships more fuel efficient. And the specific idea was, think of it as a Google Maps for ships. We can suggest a ship or tell a ship how to steer so that you still get your destination on time by using turns out about 10% less fuel. And so what we now do is we spend about a month validating the idea. So double check this is idea even technically feasible and in terms of prospective customers to make sure that this is a market need. So we spent up to about a month doing that. And if it passes this stage, then we will go and recruit a CEO to work with us on the project. When I was starting out, I used to spend a long time working on the project myself before bringing on the CEO, but after iterating, we realized that bringing on the leader at the very beginning to work with us, it reduces a lot of the burden of having to transfer knowledge or having a CO come in and have to revalidate whether we discover it. So the process is we've learned much more efficient which is bringing the leader at the very start. And so in the case of bearing AI, we found a fantastic CEO, Dylan Kyle, who's repeat entrepreneur one successful exit before and then we spent three months six two weeks sprints to work with them to go the prototype as well as do do deep customer validation. If it survives the stage and we have about a 2.36% survival rate, we never write a first check in, which then gives the company resources to hire an executive team, you know, build the key team, get the MVP working, minima viable product working, and get some real customers. And then after that, hopefully then successfully raises additional external rounds of funding that can keep on growing and scaling. So I'm really proud of the work that my team was able to do to support Mitsui's idea and Dylan Kau as CEO. And today there are hundreds of ships on the high seas right now that are steering themselves differently because of Bering AI and 10% fuel savings translates to rough water amounts to maybe $450,000 in savings and fuel per year. And of course, it's also, frankly, quite a bit better for the environment. And I think this is not a, I think would not have existed if not for the lens fantastic work. And then also, you know, Mitzi, praying this idea to me. And I like this example because this is another one. It's like, you know, this is a starting idea that I would never have come up with myself, because I've been on a boat, but what do I know about maritime shipping? But is the deep, suction-matte expertise of Mitsui that had to zunzai together with Dylan and then my team's expertise in AI that made this possible. And so as I operate an AI, one thing I've learned is my swim lane is AI, that's it because I don't have time. It's very difficult for me to be expert in maritime shipping and romantic relationships and healthcare and financial services and all and all and all. And so I've learned that if I can just help get accurate technical validation and then use AR resources to make sure the AI tech has been quickly and well. And I think we've always managed to help the companies build a strong tech and protein, then partnering with subject matter experts often results in the collecting opportunities. And I want to share with you one other weird aspect of one of the weird aspect of, and one other weird lesson of learning about building startups, which is I like to engage only when there's a concrete idea. And this runs counter to bother the advice you hear from the design thinking methodology which often says don't rush to solutioning right explore a lot of alternatives and avoid a solution, we tried that. It was very slow. But what we've learned is that at the ideation stage, if someone comes to me and says, hey, Andrew, you should apply your IIT to financial services. Because I'm not a subject matter expert in financial services is very slow for me to learn enough about financial services to even figure out what to do. I mean, eventually you could get a good outcome, but it's a very labor intensive, very slow, very expensive process with me to try to learn industry after industry. In contrast, one of my partners wrote his ideas at Tony Che cheap, not really seriously. But let's say the congares is by GBT, let's eliminate commercials by automatically buying every product advertised in exchange for not having seen ads. It's not a good idea, but it is a concrete idea. And it turns out concrete ideas can be validated or falsified efficiently. They also give a team a clear direction to execute. And I've learned it in today's world, especially with the excitement and buzz and exposure to the AI of a lot of people, it turns out that there are a lot of subject matter experts in today's world that have deeply thought about the problem for months, sometimes even one or two years, but they've not yet had a build upon there and when we get together with them and Here and they share the idea of us it allows us to work with them to very quickly go into validation and building. And I find that this works because there are a lot of people that have already done the design thinking thing of exploring a lot of ideas and winning down to really good ideas. And there are, I find that there's so many good ideas sitting out there that no one is working on that finding those good ideas that someone has already had and wants to share with us and wants to build part of the floor. That turns out to be much more efficient engine. So, before I wrap up, we'll go to the question a second. Just a few slides to talk about risk and social impact. So, it's very powerful technology to state something you pray, my team and I work on projects and move humanity forward. We have multiple times, code projects that we assess we financially sound based on ethical grounds. It turns out I've been surprised and sometimes this made a creativity of people to come up with good ideas, so to come up with really bad ideas that seem profitable but really should not be built with a few projects on those grounds. And then I think it has to be acknowledged that AI today does have problems with bias, fairness, accuracy, but also, you know, technology is improving quickly. So I see that AI systems today are less buyers than six months ago and more fair than six months ago, which is not to dismiss the importance of these problems. They are problems that we should continue to work on them. But I'm also gratified at the number of AITs working hot on these issues to make them much better. When I think of the biggest risk of AI, I think that the biggest risk, one of the biggest risk is the disruption to jobs. This is a diagram from a paper by our friend at the University of Pennsylvania and some folks at OpenAI, analyzing the exposure of different jobs to AI automation. And it turns out that whereas the previous wave of automation mainly the most The most exposed jobs are often the lower wage jobs, such as when we put robots into factories with this current wave of automation is actually the higher-wage shops further the right of this axis that seems to So even as we create tremendous value using AI, I feel like as citizens and corporations and the governments and really our society, I feel a strong obligation to make sure that people, especially people who are lively, who are disrupted, are still well taken care of, are still treated well. And then lastly, there's also been, it feels like every time there's a big wave of progress in AI, there's a big wave of hype about artificial gender intelligence as well. When deep learning starts to work really well 10 years ago, there was a lot of hype about AGI and now the gen of AGI is working really well. There's another wave of hyperbubble AGI. But I don't think there's any time soon. One of the challenges is that the biological path to intelligence, like humans and the paths intelligence, you know AI, they've taken very different paths and the funny thing about the definition of AGI is, benchmarking, there's very different digital parts intelligence with really the biological parts intelligence. So I think, you know, Larsson Kraschmoldo is as smarter than any of us in certain key dimensions, but much dumber than any of us in other dimensions. And so forcing it to do everything a human can do is like a funny comparison. But I hope we'll get there, maybe hopefully within a lot of times. And then there's also a lot of, I think think overblown hype about AI creating extinction risk for humanity. Candidly, I don't see it. I just don't see how AI trees, any meaningful extinction risk of humanity. I think that people worry we can't control AI but we have lots of AI will be more powerful than any person but with lots of experience steering very powerful entities such as corporations or nation states that are far more powerful than any single person and making sure they for the most part benefit humanity. And also technology develops gradually. The so-called hot takeoff scenario, where it's not really working today and suddenly one day overnight it works brilliantly and we achieve super intelligence in six over a world. That's just not realistic. And I think the AI technology would develop slowly, like all the technology and then it gives us plenty of time to make sure that we provide oversight and can manage it to be safe. And lastly, if you look at the real extinction of rest of humanity, such as fingers crossed an expendemic or climate change leading to a massive depopulation of some parts of the planet or much lower odds but maybe someday an asteroid doing to us what it had done to the dinosaurs. I think we look at the actual growing extinction risk humanity AI having more intelligence, even artificial intelligence in the world, will be a key part of the solution. So I feel like if you want humanity to survive and thrive for the next thousand years rather than slowing AI down which some people propose I would rather make AI go as fast as possible. Some of that just summarizes my last slide. I think that AI as a general purpose technology creates a lot of new opportunities for everyone and a lot of the exciting and important work that lies ahead of us all is to go and build those concrete use cases and hopefully in the future, hopefully I have opportunities to maybe engage with more of you on those opportunities as well. So that, let me just say thank you all very much. Thank you.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 7425 tokens (7169 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\DataSquad_Tasks\\summarizer\\notebooks\\summarize.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataSquad_Tasks/summarizer/notebooks/summarize.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarize\u001b[39;00m \u001b[39mimport\u001b[39;00m load_summarize_chain\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DataSquad_Tasks/summarizer/notebooks/summarize.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m chain \u001b[39m=\u001b[39m load_summarize_chain(model, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmap_reduce\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DataSquad_Tasks/summarizer/notebooks/summarize.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m caption \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49mrun(documents)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\chains\\base.py:451\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    450\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[0;32m    452\u001b[0m         _output_key\n\u001b[0;32m    453\u001b[0m     ]\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[0;32m    457\u001b[0m         _output_key\n\u001b[0;32m    458\u001b[0m     ]\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\chains\\base.py:258\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 258\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    259\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    260\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    261\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    262\u001b[0m )\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\chains\\base.py:252\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    246\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    247\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    248\u001b[0m     inputs,\n\u001b[0;32m    249\u001b[0m )\n\u001b[0;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    253\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    254\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:106\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    105\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m--> 106\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[0;32m    107\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    109\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:210\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_docs\u001b[39m(\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    200\u001b[0m     docs: List[Document],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    204\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m \u001b[39m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     map_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m    211\u001b[0m         \u001b[39m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[0;32m    212\u001b[0m         [{\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_variable_name: d\u001b[39m.\u001b[39;49mpage_content, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs} \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m docs],\n\u001b[0;32m    213\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    214\u001b[0m     )\n\u001b[0;32m    215\u001b[0m     question_result_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39moutput_key\n\u001b[0;32m    216\u001b[0m     result_docs \u001b[39m=\u001b[39m [\n\u001b[0;32m    217\u001b[0m         Document(page_content\u001b[39m=\u001b[39mr[question_result_key], metadata\u001b[39m=\u001b[39mdocs[i]\u001b[39m.\u001b[39mmetadata)\n\u001b[0;32m    218\u001b[0m         \u001b[39m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    219\u001b[0m         \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(map_results)\n\u001b[0;32m    220\u001b[0m     ]\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\chains\\llm.py:186\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    185\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    187\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n\u001b[0;32m    188\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end({\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: outputs})\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\chains\\llm.py:183\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    178\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    179\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    180\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39minput_list\u001b[39m\u001b[39m\"\u001b[39m: input_list},\n\u001b[0;32m    181\u001b[0m )\n\u001b[0;32m    182\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    184\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    185\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\chains\\llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[0;32m    103\u001b[0m     prompts,\n\u001b[0;32m    104\u001b[0m     stop,\n\u001b[0;32m    105\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    106\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[0;32m    107\u001b[0m )\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\llms\\base.py:451\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    444\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    445\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    449\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    450\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\llms\\base.py:582\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         )\n\u001b[0;32m    576\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    577\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    578\u001b[0m             dumpd(\u001b[39mself\u001b[39m), [prompt], invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[0;32m    579\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    580\u001b[0m         \u001b[39mfor\u001b[39;00m callback_manager, prompt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(callback_managers, prompts)\n\u001b[0;32m    581\u001b[0m     ]\n\u001b[1;32m--> 582\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[0;32m    583\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    584\u001b[0m     )\n\u001b[0;32m    585\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\llms\\base.py:488\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    487\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 488\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    489\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    490\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\llms\\base.py:475\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    467\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    472\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    473\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 475\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[0;32m    476\u001b[0m                 prompts,\n\u001b[0;32m    477\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    478\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    479\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    480\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    481\u001b[0m             )\n\u001b[0;32m    482\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    483\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    484\u001b[0m         )\n\u001b[0;32m    485\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    486\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\llms\\openai.py:399\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    388\u001b[0m         {\n\u001b[0;32m    389\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m         }\n\u001b[0;32m    397\u001b[0m     )\n\u001b[0;32m    398\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[0;32m    400\u001b[0m         \u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    403\u001b[0m     update_token_usage(_keys, response, token_usage)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\llms\\openai.py:115\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\concurrent\\futures\\_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    436\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\concurrent\\futures\\_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\langchain\\llms\\openai.py:113\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32md:\\DataSquad_Tasks\\summarizer\\env\\lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 7425 tokens (7169 in your prompt; 256 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(model, chain_type=\"map_reduce\", verbose=True)\n",
    "caption = chain.run(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a1165d-214c-4a6c-95d9-f7e8f90e8ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This video explains how to do file handling and organization in Python, including how to navigate directories, rename, move, copy, and remove files and directories. It also covers the OS and pathlib modules, the shutil module, and how to run a Python script as a cron job. A real-life example of an automation script is provided.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65237526-a949-4502-b8c7-815342ce6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"You are an AI assistant which generates a summary from a transcript text of a video. \\\n",
    "The following is the transcript text generated from a video. Creating meaningful summary of the \\\n",
    "the transcript is your job. Based on the summary, anyone can learn the topics covered in the video. \n",
    "The summary should not be just a caption which only describes about the content of the video. You can create \\\n",
    "seperate points to demonstrate topics coverd in the video and which all things are discussed in that topic; write that inside \\\n",
    "that particular points. In short, the summary should be precised and descriptive unlike a shorter caption.\n",
    "You are not supposed to answer any other questions.\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "Transcript: {text}\n",
    "Summary: \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14490634-c489-4fd2-a213-77559047e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompts.video_summarizer_prompt import VideoSummarizerPromptTemplate\n",
    "\n",
    "prompt = VideoSummarizerPromptTemplate(\n",
    "    prefix=prefix,\n",
    "    suffix = suffix,\n",
    "    input_variables = [\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8478b46b-b02a-452a-bf5b-c8e33234cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = prompt.format(\n",
    "    text = documents[0].page_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a703fa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a5929d-c7e3-48ff-8b29-80da9f9a78cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant which generates a summary from a transcript text of a video. The following is the transcript text generated from a video. Creating meaningful summary of the the transcript is your job. Based on the summary, anyone can learn the topics covered in the video. \n",
      "The summary should not be just a caption which only describes about the content of the video. You can create seperate points to demonstrate topics coverd in the video and which all things are discussed in that topic; write that inside that particular points. In short, the summary should be precised and descriptive unlike a shorter caption.\n",
      "You are not supposed to answer any other questions.\n",
      "\n",
      "\n",
      "Transcript:  What's up everyone? In this video I show you how to do file handling and file organization in Python. So I show you how to navigate to different directories how to rename files, move them, copy them or remove files and directories. So these are all the functions we need in order to automate our file organization. And then I also show you one real life example. So I show you one automation script that I use in order to keep my desktop organized. So let's get started. First of all, let's learn how we can navigate to a different directory and for this we use the OS module and first of all we can print the current working directory by calling os.getcvd and this will print this path so this is where the main.py file is located and now let's switch to a different directory. So here on the desktop I have one folder that I called video files where I have a couple of files in it. So now I want to change into this directory and I can do this by calling os change der and then let's put in this path. So I want users Patrick desktop and then it's called video files. And now if I print the current working directory afterwards, then let's run this and now we see we are in this directory. And now the next thing I want to do is to rename these files so that I can sort them according to the name. For example, I want the numbers to be in the beginning and then there's also some spaces that I want to remove so first of all we can call os.list. And let's remove this print statement and if we run this then we get a list with all the files in this directory so we can iterate over this so we can say four file in OS list there and then I want to print the file and if we run this then we get all the different file names And then we have this one that we don't even see here because it's starting with a dot. So I want to ignore this so I can say if file equals equals this name, then I want to continue and otherwise print the file and then we should only get these three files. Now the next thing I want to do is to split the base name from the extension and then modify the name and put it back together with the extension afterwards. And the way to do this is saying name and x equals os dot path dot split x and it needs the file name. And now if we print the name and then print the extension in a separate line and run this, then we see we now split it the extension from the file name. So now let's modify the file name and create a new one and I want to have the number in the beginning then I want Python course and then I want this title in the end. And I also want to get rid of the spaces. So the way to do this is for example we can split this string at each dash and then get all the different elements. So we say split it equals name dot split at a dash. So this is a function that we can use on a string and if we print this then we see this will be a list. So this is a list now with all the separate elements. So now the next thing I want to do is to get rid of spaces like here for each element. And we can do this with list comprehension. So we say split it equals s dot strip for s in split it. So for each string in this list, we call s dot strip. And now if we run this, then we no longer have spaces in here. And now let's create a new name so we say new name equals and now we use an F string and now we put in the different elements in curly braces and create our new name. So we say curly braces dash curly braces dash curly braces dash and then we need one more. So in this very last field we put in our extension and here we want the number. So this is split it of element of index three. Then we put this for the other ones as well. So this is split it with element 1, this should be Python. Then here split it to this is the course and here we have the title so this is split at zero so now if we print the new name, then we see our name now looks like this. And now I want to have one more improvement so here I always want to have two digits in the beginning and for this I can call the handy cfil method and specify to as the argument. And now if we run this, then this is our final new file name. And now we only have to rename the file and for this we can call os.rename and then we need to put in the old file name. So this is the file and then the new name. And now let's watch the right side. So now I will execute this. And you see it immediately changed all the file names and renamed them. Now before we move on to moving files, I also want to show you how we can do this the modern Python way. So let's copy the old filenames into this again. And we can also say we say from pathlib we want to import the path and then we create a path object by saying f equals path and here we pass in the file and now we can say name equals f dot stem and the extension is f dot suffix. So this will be the same and then we say f dot rename to the new name and now again, let's watch the right side and run this and you see this work as well with the new syntax using the pathlib module. Now the next thing I want to do is to move a file and for this let's get rid of all of this and now the first thing I want to do is to create a new directory and we can do this by calling path and then specify a new folder name and then we call make sure and we use Xists okay equals true then it will not raise an exception if this already exists. So now if we run this, then it created a new data directory. We could also do it the old way by saying if not OS path exists and the name is data then we call OS make their data and now here I deleted it again and now if we comment this out and run this then here it worked as well. So now we have to be careful because now if we print all the files, you will see that now we also have this data in Lister, so it not only iterates over the files, but also over directories. So here we also have to say our file equals equals data now. And now in order to move a file, we import the SHU tool module and then we simply call SHUTIL.Move and then we specify the source file and now the destination is data. So this can take a whole file name but also a directory name like so. And now if we run this then you see it moved all the files into this directory. Then let's learn how to copy files instead of moving them. So let's move them out again here. And now in order to copy them, we can use SHUtil.copy and if we run this then you see it now created new copies in this directory. Now this has a the timestamp of right now. And if you want to keep the old timestamps and all other meta tags, then you can use the copy to function. So if I remove these again and now run the file then you see now we have copies and we have the same old timestamp. So this is how you copy and it keeps all the metadata. And now it's last thing, let's learn how we can delete a file and folder again. So for this, we can use os.remove and then use a file name. We can also use os.rm there and then a folder name for example data but if we run this then it will not work in our case because now we get an error directory is not empty. So in order to remove a non-empty directory, we can again use the SSHU tool module and then call RM3 and now here data. So this will recursively delete a directory tree. And if we run this, then it worked and now the whole directory is gone. And now the last thing I want to show you is this backup script that I use in order to keep my desktop organized. So as you can see there are a lot of different files that get collected over time, for example movies or audio files or images and a lot of screenshots. So here I defined all the possible file endings for audio files and the same for videos and images. And then I have helper functions that determine if a file is an audio file and it works by using the split x function and then it checks if the extension is in the audio extensions and the same for videos and images and then I even have a helper function that checks if it's a screenshot. So for a screenshot it has to be an image and then also screenshot has to be in name.lower and then I change into my desktop and then I iterate over os.lister and then I call the different helper functions and then I copy or here I move the files into the corresponding folders that I want to use. For example, for an audio file, it moves the file into this directory and then the same for videos, images, screenshots and all other documents. And this is basically all that I use in order to keep my desktop clean and then in order to automate this you can run this script every once in a while as a cron job and I have a full tutorial on my channel that explains how to run a Python script as a cron job. So check that out if you're interested and this is all I wanted to show you in this tutorial I hope you enjoyed this and then I hope to see you in the next video. Bye\n",
      "Summary: \n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bf6d4e0-7b80-4e0f-953f-2cbebd5f4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = model,\n",
    "    prompt = prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a419774-367a-431a-9009-fc0819e14c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This video demonstrates how to do file handling and file organization in Python using the OS and pathlib modules. It covers topics such as navigating through different directories, renaming files, moving files, copying files, and removing files and directories. It also provides a real-life example of how to use an automation script to keep a desktop organized. Finally, it shows how to use a cron job to automate the script.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c2976f-c867-4e6e-902d-3de60863136e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMachine Learning is a subset of Artificial Intelligence which enables machines to learn from data, identify patterns, and make decisions without explicit programming. It is used to develop applications that can analyze large datasets to make predictions, automate tasks, and provide recommendations. This technology is used in many industries such as healthcare, finance, and retail. In this video, the concept of Machine Learning is discussed in detail, including the types of machine learning algorithms, the different types of data used for machine learning, and the potential applications of machine learning.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f55d53f4-e443-4efc-9e5c-b1818331fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "map_template = \"\"\"The following is a set of documents containing a transcript of an youtube video.\n",
    "{docs}\n",
    "Based on this list of docs, please generate the summary of the transcripts.\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "map_chain = LLMChain(llm=model, prompt=map_prompt)\n",
    "\n",
    "reduce_template = \"\"\"The following is set of summaries of a transcripted video:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of that particular video transcript. \n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "reduce_chain = LLMChain(llm=model, prompt=reduce_prompt)\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "                    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    "                    )\n",
    "\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "                combine_documents_chain=combine_documents_chain,\n",
    "                collapse_documents_chain=combine_documents_chain,\n",
    "                token_max=4000,\n",
    "                )\n",
    "\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "                llm_chain=map_chain,\n",
    "                reduce_documents_chain=reduce_documents_chain,\n",
    "                document_variable_name=\"docs\",\n",
    "                return_intermediate_steps=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0ab974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" It is my pleasure to welcome Dr. Andrew Wu tonight. Andrew is the Managing General Partner of AI Fund, founder of Deep Learning AI and lending AI, chairman and co-founder of Coursera, and an unjunct professor of computer science here at Stanford. Previously he had started and led the Google Brain team, which had helped Google adopt modern AI. And he was also director of the Stanford AI lab. About 8 million people, one in 1000 persons on the planet have taken an AI class from him and through both his education and his AI work. He has changed humor's lives. Please welcome Dr. An. Thank you Lisa. It's good to see everyone. So what I want to do today is chat new electricity. One of the difficult things to understand about AI is that it is a general purpose technology. Meaning that it's not useful only for one thing, but it's useful for lots of different applications. Kind of like electricity. If I were to ask you what is electricity good for? You know, it's not any one thing, there's a lot of things. So what do I like to do is start off sharing with you how I view the technology landscape and just to lead into the set of opportunities. So, a lot of hype, a lot of excitement about AI. And I think a good way to think about AI is as a collection of tools. So this includes a technique called supervised learning, which is very good at recognizing things or labeling things, and genitive AI, which is relatively new, exciting development. If you're familiar with AI, you may have heard of other tools, but I'm going to talk less about these additional tools. And I'll focus today on what I think are currently the two most important tools which are supervised learning and generative AI. So supervised learning is very good at labeling things or very good at computing input to outputs or A to B mappings given input A, give me an output B. For example, given an email, we can use supervised learning to label a spam or not spam. The most lucrative application of this that I've ever worked on is Pravee Online advertising where I give it an ad we can label the users likely to click on it and therefore show more relevant ads. For self-driving calls given the sense of readings of a car we can label it with where the other cars One project that my team at AFN worked on was shift route optimization. We're given a route to the ship is taking or considering taking. We can label that with how much fuel we think is to consume and use this chips more fuel efficient. The law work in automated visual inspection in factories. So you can take a picture of a smartphone that was just manufactured and labeled as a scratch or any other defect in it. Or if you want to build a restaurant review reputation monitoring system, you can have little piece of software that looks at online restaurant reviews and labels that as positive or negative sentiment. So one nice thing, one cool thing about supervised learning is that it's not useful for one thing. It's useful for all of these different applications and many more besides. Let me just walk through, you know, concretely, the workflow of one example of a supervised learning labeling things kind of project. If you want to build a system to label restaurant reviews, you then collect a few data points, so collect the data set where it say, you know, the best strategy sandwich great to say that this positive circles is slow. That's negative. My favorite should be curry is positive. And here I've shown three data points, but you're building this, you may get thousands of data points like this, or thousands of training examples, we call it. And the workflow of a machine learning project, when AI project is you get labeled data, maybe thousands of data points, then you have an AI entry team train an AI model to learn from this data. And then finally, you would find maybe a cloud service to run the trained I model and they can feed it, you know, less multi-either-vahad and less positive sentiment. And so I think the launch decade was maybe the decade of large scale supervised learning. What we found starting about 10, 15 years ago was if you were to train a small AI model, so train a small neural network with small deep learning algorithm, basically a small AI model, maybe not on a very powerful computer. Then as you've had more data, as performance, would get better for a little bit, but then it would flatten out, it would plateau, and it would stop being able to use the data to get better and better. But if you were to train a very large AI model, lots of compute on maybe powerful GPUs. Then as we scaled up the amount of data we gave the machine learning model, its performance would kind of keep on getting better and better. So this is why when I started and led the Google Brain team, the primary mission that I directed the team to solve at the time was, let's just build really, really large neural networks that we then fed a lot of data to and that recipe fortunately worked. And I think the idea of driving large compute and large scale data that recipes really helped us driven a lot of AI progress over the last decade. So if that was the last decade of AI, I think this decade is turning out to be also doing everything we had in supervised learning, but adding to it the exciting tool of Genese AI. So, many of you, maybe all of you, were played with charge-gbd and bar and so on. But just, you know, given a piece of text, which we call prompt. I love eating. If you run this multiple times, maybe you get bigger screen keys or my mother's me love or outer friends and the AI system can generate output like that. Given the amounts of buzz and excitement about genesis of AI, I thought I'd take just half a slide to see a little bit about how this works. So it turns out that, at least this type of text generation, the core of it is using supervised learning that inputs output mappings to repeatedly predict the next word. And so if your system reads on the internet to sentence like, my favorite food is a bagel with cream cheese and locks, then this is translated into a few data points where if it sees my favorite food is a in just case try to guess that the right next word was bagel or my favorite food is a bagel try to guess next word is worth and similarly you've a season So by taking text that you find on the internet or other sources and by using this input output supervised, to try to repeatedly predict the next word. If you train a very large AI system on hundreds of billions of words, or in the case of the largest model is now more than a trillion words, then you get a large language model like Chaggb. And, you know, there are additional other important technical details I talked about predicting the next word. Technically, these systems predict the next subword, a part of work called token, and then there are other techniques like RHSF for further tuning the air output to be more helpful on this and harmless. But at the heart of it is this using supervised learning to repeatedly predict the things where that that's really was enabling the exciting, you know, really fantastic progress on large language models. So, while many people have seen large language models as a fantastic consumer too, You can go to a website like check TV's website or bots or other launch language malls and users. I think it's fantastic too. There's one of the trends I think is still underappreciated, which is the power of large language models, not as it, not just as consumer to, but as it develop a to. So it turns out that there are applications that used to take me months to build that a lot of people can now build much faster by using a large language model. So specifically, the workflow for supervised learning, building the restaurant review system, say, would be that you need to get a bunch of labeled data, and maybe that takes a month to get a few thousand data points. And then have an AI team train and tune and really get optimized performance on your AI model, maybe that would take three months. Then find a cloud service to run it, make sure it's running robustly, make sure it's recognized, maybe that would take another three months. So pretty realistic timeline for building a commercial grade machine learning system is like six to 12 months. So teams I've led're often took roughly six or 12 months to build and deploy these systems and some of them turned out to be really valuable, but this is a realistic timeline for building and deploying a commercial-grade AI system. In contrast, with prompt base AI, where you write a prompt, this is what the workflow looks like. You can specify a prompt that takes maybe minutes or hours and then you can deploy it to the cloud and that takes maybe hours or days. So there are certain AI applications that used to take me literally six months, maybe a year to build that many teams around the world can now build in maybe a week. And I think this is already starting, but the best is still yet to come. This is starting to open up a flood of a lot more AI applications that can be built by a lot of people. So I think many people still underestimate the magnitude of the flood of customer applications that I think is going to come down the pipe. Now, I know you probably were not expecting me to write code in this presentation, but that's what I'm going to do. So it turns out this is all the code I need in order to write a sentence in classifier. So I'm gonna, you know, solve you with no Python, I guess, import some tools from OpenAI, and then I have this prompt that says classified detects the low, delimited by three dashes is having either a positive or negative sentiments. I don't know. to have a fantastic time Learned a lot and also made great new friends. So that's my problem. And now I'm just going to run it. And I've never run it before. So I really hope that thank goodness we got the right answer. And this is literally all the code it takes the build a sentiment classifier. And so today, you know, developers around the world can take literally maybe like 10 minutes to build a system like this. And that's a very exciting development. So one of the things I've been working on was trying to teach online classes about how to use prompt promoting, not just as a consumer, but as a developer too. So, just about the technology landscape. Let me now share my thoughts on what are some of the AI opportunities I see. This shows what I think is the value about three years from now. But the vast majority of financial value from AI today is I think supervised learning where for a single company like Google can be worth more than $100 billion a year. And also, there are millions of developers building supervised learning applications. So it's already massively valuable and also with tremendous momentum behind it just because of the sheer effort in finding applications and building applications. And in Genose Bay, I is the really exciting new entrance, which is much smaller right now. And then there are the other tools's I'm including for completeness we can you know If the size of these circles represent the value today, this is what I think it might grow to in three years. So supervised learning already really massive may double say in the next three years, from truly massive to even more massive. And James Sabei, which is much smaller today, I think we'll much more than double in the next three years because of the number of amounts of developer interest, the amount of venture capital investments and large corporate exploring applications. And I also just want to point out, three years is a very short time horizon. If it continues to compound at anything near this rate, then in six years,, will be even faster larger. But just light shaded region in green or orange, that light shaded region is where the opportunities for either new startups offer large companies and companies to create and create a value capture. technologies are general purpose technologies. So in the case of supervised learning, a lot of the work that had to be done over the last decade, but it's continuing for the next decade is to identify and to execute on the concrete use cases. And that process is also kicking off for Genes of AI. So for this part of the presentation, I hope you take away from it that general purpose technologies are useful for many different tasks. A lot of value remains to be created using supervised learning and even though we're nowhere near finishing figure out exciting use cases of supervised learning where there's other fantastic two of genus avi which further expands the set of things we can now do using a i But one caveat, which is that there will be short-term fads along the way. So I don't know if some of you might remember the app called LENZA. This is the app that will let you upload pictures of yourself and then render a cool picture of you as an astronaut or a scientist or something. And it was a good idea and people liked it. And it's rough, it's just so cold like crazy like that through last December. And then it did that. And that's because lensa was, it was a good idea, people liked it, but it was a relatively thin software layer on top of someone else's really powerful APIs. And so even though it was a useful product, It was in a defensive all-possess. And when I think about, you know, absolute lenser. I'm actually reminded that when Steve Jobs gave us the iPhone, shortly after someone wrote an app that I paid $1.199 for to turn on the LED, to turn the phone into flashlight. And that was also a good idea to write an app to turn on the LED light, but it was a defensible long turn. It also didn't create very long-term value because it was a easy replicated and underpriced and eventually incorporated into iOS. But with the rise of iOS, with the rise of iPhone, someone also figured out how to build things like Uber and Airbnb and Tinder. The very long term, very defensible businesses that created, you know, sustaining value. And I think with the rise of gender AI or the rise of new AI tools, I think what really excites me is the opportunity to create those really deep, really hard applications that hopefully can create very long-term value. So the first trend I want to share is as a general purpose technology and a lot of work that lies ahead of us is to find the very diverse use cases and to build them. This is a second trend I want to share with you. We'll release the YA AI isn't more widely adopted yet. It feels like a bunch of us have been talking about AI for like 15 years or something, but if you look at where the value of AI is today, a lot of it is still very concentrated and consumer-software internet. Once you go outside, you know, tech or consumer software internet, there's some air adoption but the law views very early. So why is that? It turns out if you were to take all current and potential AI projects and sought them in decreasing order value. Then to the left of this curve, the head of this curve are the multi billion dollar project the for e-commerce, product recommendations, or company Amazon. It turns out that about 10, 15 years ago, various of my friends and I, we figured out a recipe for how to hire, say, a hundred engineers to write one piece of software to serve more relevant ads and apply that one piece of software to billion users and generate massive financial value. So that works. But once you go outside, consume a software internet, how do anyone has 100 million or a billion users, they can write and apply one piece of software to you. So once you go to other industries as we go from the head of this curve on the left over to the long tail, these taking pictures of the piece they were making because they needed to do things like make sure that the cheese is spread evenly. So this is about a $5 million project, but that recipe of hiring 100 engineers or dozens of engineers to work on a five million dollar project that doesn't make sense. Or another example, working with an agriculture company that would then we figured out that we used cameras to find out how tall is the wheat. A wheat is often bento because of wind or rain or something. And we can chop off the wheat at the right height, then that results in more food for the farmer to sell, and it's also better for the environment. But this is another $5 million project that owed recipe of having a large group of high school engineers to work on this one project that doesn't make sense. And some of the materials grading, cloth grading, sheet metal grading, many project like this. So whereas to the left in the head of this curve there's a small number of let's say multi-billion dollar projects and we know how to execute those delivering value. In other industries, I'm seeing a very long tail of tens of thousands of, let's call them, five million dollar projects, then until now, it had been very difficult to excuse on because of the high cost of customization. The trend that I think is exciting is that the AI community has been building better tools that lets us aggregate these use cases and make it easy for the end user to do the customization. So specifically, I'm seeing a lot of exciting low code and no code tools that enable the user to customize the AI system. What this means is instead of me needing to worry that much about pictures of pizza. We have tools, we can start into C2s that can enable the IT department and the PISA making factory to train AI systems on their own pictures of PISA to realize this $5 million worth of value. And by the way, the pictures of Pizzer, they don't exist on the internet. So Google and Bing don't have access to these pictures. We need tools that can be used by really the piece of factory themselves to build and deploy and maintain their own custom AI system that works on their own pictures of PISA. And broadly, the technology for enabling this. Some of it is prompting. Text prompting, visual prompting, but really large language models and similar tools like that. Or a technology called data centcentric AI whereby instead of asking the piece of factory to write a lot of code, which is challenging. We can ask them to provide data, which turns out to be more feasible. And I think the second trend is important because I think this is a key part of the recipe for taking the value of AI, which so far still feels very concentrated in the tech world and consumer software into that world and pushing this out to all industries really to the rest of the economy, which sometimes is easy to forget. The rest of the economy is much bigger than the tech world. So, R2 trends are shared as a general purpose technology technology lots of compute use cases to be realized as well as local no code easily used tools enabling AI to be deployed in more industries. How do we go after these opportunities? So about five years ago, there was a puzzle I wanted to solve, which is I felt that many valuable AI projects are now possible. I was thinking, how do we get them done. And having led AI teams in Google and by do in big tech companies, I had a hard time figuring out how I could operate a team in a big tech company to go off. There are very diverse opportunities and everything from maritime shipping to education to financial services, the healthcare and all that. It's just very diverse use cases, very diverse, go to markets and very diverse, really, you know, customer bases and then applications. And I felt that the most efficient way to do this would be we can start a lot of different companies to pursue these very diverse opportunities. So that's why I end up starting AI fun, which is a venture studio that builds startups to pursue a diverse set of our opportunities. And of course, in addition to lots of startups in company companies also have a lot of opportunities to integrate AI into existing businesses. In fact, one pattern I'm seeing for incumbent businesses is distribution. distribution is often one of the cyclical advantages of incoming companies that they play the cards right can allow them to integrate So, I think of this as a, this is why I think of it as the AI stack. At the bottom level is the hardware semiconductor layer. Fantastic opportunities there but very capital intensive, very concentrated. So there's a lot of resources around the few winners. So some people can and should play there. I personally don't like to play there myself. There's also the infrastructure layer. Also fantastic opportunities, but very capital intensive, very concentrated, so I tend not to play that myself either. And then there's a developer tool later. What I showed you just now was I was actually using OpenAI's API as a developer tool. And then I think the developer tool sector is a hyper competitive. Look at all the startups chasing OpenAI right now, but there will be some mega winners. And so I sometimes play here, but primarily when I think of a meaningful technology advantage because I think that earns you the right or earns you a better shot at being one of the mega winners. And then lastly, even though a lot of the media attention in the buzz is in the infrastructure and developer tooling layer, it turns out that that layer can the application layer is even more successful. And we saw this with the rise of SAS as well. A lot of the buzz and excitement is on the technology, the tooling layer, which is fine, nothing wrong with that. But the only way for that to be successful is that the application layer is even more successful so that frankly they can generate enough revenue to pay the infrastructure and the tooling layer. So, let me mention one example. ArmorRy, it's actually just texting the CO yesterday. ArmorRy is a company that we built that uses AI for romantic relationship coaching. And just to point out, I'm an AI guy and I feel like I know nothing really about romance. And if you don't believe me, you can ask my wife if she will confirm that I know nothing about romance. But we want to build this. We want to get together with the former CEO of Tinder, a Renata Nibalb, and with my team's expertise in AI and her expertise in relationships. When she ran Tinder, she knows more about relationships. I think anyone I know, were able to build something pretty unique using AI for romantic relationship mentoring. The interesting thing about applications like these is when we look around, how many teams in the world are simultaneously expert in AI and in relationships. And so at the application layer, I'm seeing a lot of exciting opportunities that seem to the very large market, but where the competition set is very light relative to the magnitude of the opportunity. It's not that there are no competitors, but it's just much less intense compared to the developer tool of the infrastructure of the era. And so because I've spent a lot of time iterating on a process of building startups. What we're going to do is just very transparent to tell you the recipe we've developed for building startups. And so after many years of iteration and improvement, this is how we now build startups. My team's always had accesss a lot of different ideas, your internal generated ideas from partners, and I want to walk through this with one example or something we did, which is a company bearing AI, which uses AI to make ships more fuel efficient. So this idea came to me when a few years ago a large Japanese conglomerate calledui, that is a major shareholder in the opera's major shipping lines. They came to me and they said, hey, Andrew, you should build a business to use AI to make ships more fuel efficient. And the specific idea was, think of it as a Google Maps for ships. We can suggest a ship or tell a ship how to steer so that you still get your destination on time by using turns out about 10% less fuel. And so what we now do is we spend about a month validating the idea. So double check this is idea even technically feasible and in terms of prospective customers to make sure that this is a market need. So we spent up to about a month doing that. And if it passes this stage, then we will go and recruit a CEO to work with us on the project. When I was starting out, I used to spend a long time working on the project myself before bringing on the CEO, but after iterating, we realized that bringing on the leader at the very beginning to work with us, it reduces a lot of the burden of having to transfer knowledge or having a CO come in and have to revalidate whether we discover it. So the process is we've learned much more efficient which is bringing the leader at the very start. And so in the case of bearing AI, we found a fantastic CEO, Dylan Kyle, who's repeat entrepreneur one successful exit before and then we spent three months six two weeks sprints to work with them to go the prototype as well as do do deep customer validation. If it survives the stage and we have about a 2.36% survival rate, we never write a first check in, which then gives the company resources to hire an executive team, you know, build the key team, get the MVP working, minima viable product working, and get some real customers. And then after that, hopefully then successfully raises additional external rounds of funding that can keep on growing and scaling. So I'm really proud of the work that my team was able to do to support Mitsui's idea and Dylan Kau as CEO. And today there are hundreds of ships on the high seas right now that are steering themselves differently because of Bering AI and 10% fuel savings translates to rough water amounts to maybe $450,000 in savings and fuel per year. And of course, it's also, frankly, quite a bit better for the environment. And I think this is not a, I think would not have existed if not for the lens fantastic work. And then also, you know, Mitzi, praying this idea to me. And I like this example because this is another one. It's like, you know, this is a starting idea that I would never have come up with myself, because I've been on a boat, but what do I know about maritime shipping? But is the deep, suction-matte expertise of Mitsui that had to zunzai together with Dylan and then my team's expertise in AI that made this possible. And so as I operate an AI, one thing I've learned is my swim lane is AI, that's it because I don't have time. It's very difficult for me to be expert in maritime shipping and romantic relationships and healthcare and financial services and all and all and all. And so I've learned that if I can just help get accurate technical validation and then use AR resources to make sure the AI tech has been quickly and well. And I think we've always managed to help the companies build a strong tech and protein, then partnering with subject matter experts often results in the collecting opportunities. And I want to share with you one other weird aspect of one of the weird aspect of, and one other weird lesson of learning about building startups, which is I like to engage only when there's a concrete idea. And this runs counter to bother the advice you hear from the design thinking methodology which often says don't rush to solutioning right explore a lot of alternatives and avoid a solution, we tried that. It was very slow. But what we've learned is that at the ideation stage, if someone comes to me and says, hey, Andrew, you should apply your IIT to financial services. Because I'm not a subject matter expert in financial services is very slow for me to learn enough about financial services to even figure out what to do. I mean, eventually you could get a good outcome, but it's a very labor intensive, very slow, very expensive process with me to try to learn industry after industry. In contrast, one of my partners wrote his ideas at Tony Che cheap, not really seriously. But let's say the congares is by GBT, let's eliminate commercials by automatically buying every product advertised in exchange for not having seen ads. It's not a good idea, but it is a concrete idea. And it turns out concrete ideas can be validated or falsified efficiently. They also give a team a clear direction to execute. And I've learned it in today's world, especially with the excitement and buzz and exposure to the AI of a lot of people, it turns out that there are a lot of subject matter experts in today's world that have deeply thought about the problem for months, sometimes even one or two years, but they've not yet had a build upon there and when we get together with them and Here and they share the idea of us it allows us to work with them to very quickly go into validation and building. And I find that this works because there are a lot of people that have already done the design thinking thing of exploring a lot of ideas and winning down to really good ideas. And there are, I find that there's so many good ideas sitting out there that no one is working on that finding those good ideas that someone has already had and wants to share with us and wants to build part of the floor. That turns out to be much more efficient engine. So, before I wrap up, we'll go to the question a second. Just a few slides to talk about risk and social impact. So, it's very powerful technology to state something you pray, my team and I work on projects and move humanity forward. We have multiple times, code projects that we assess we financially sound based on ethical grounds. It turns out I've been surprised and sometimes this made a creativity of people to come up with good ideas, so to come up with really bad ideas that seem profitable but really should not be built with a few projects on those grounds. And then I think it has to be acknowledged that AI today does have problems with bias, fairness, accuracy, but also, you know, technology is improving quickly. So I see that AI systems today are less buyers than six months ago and more fair than six months ago, which is not to dismiss the importance of these problems. They are problems that we should continue to work on them. But I'm also gratified at the number of AITs working hot on these issues to make them much better. When I think of the biggest risk of AI, I think that the biggest risk, one of the biggest risk is the disruption to jobs. This is a diagram from a paper by our friend at the University of Pennsylvania and some folks at OpenAI, analyzing the exposure of different jobs to AI automation. And it turns out that whereas the previous wave of automation mainly the most The most exposed jobs are often the lower wage jobs, such as when we put robots into factories with this current wave of automation is actually the higher-wage shops further the right of this axis that seems to So even as we create tremendous value using AI, I feel like as citizens and corporations and the governments and really our society, I feel a strong obligation to make sure that people, especially people who are lively, who are disrupted, are still well taken care of, are still treated well. And then lastly, there's also been, it feels like every time there's a big wave of progress in AI, there's a big wave of hype about artificial gender intelligence as well. When deep learning starts to work really well 10 years ago, there was a lot of hype about AGI and now the gen of AGI is working really well. There's another wave of hyperbubble AGI. But I don't think there's any time soon. One of the challenges is that the biological path to intelligence, like humans and the paths intelligence, you know AI, they've taken very different paths and the funny thing about the definition of AGI is, benchmarking, there's very different digital parts intelligence with really the biological parts intelligence. So I think, you know, Larsson Kraschmoldo is as smarter than any of us in certain key dimensions, but much dumber than any of us in other dimensions. And so forcing it to do everything a human can do is like a funny comparison. But I hope we'll get there, maybe hopefully within a lot of times. And then there's also a lot of, I think think overblown hype about AI creating extinction risk for humanity. Candidly, I don't see it. I just don't see how AI trees, any meaningful extinction risk of humanity. I think that people worry we can't control AI but we have lots of AI will be more powerful than any person but with lots of experience steering very powerful entities such as corporations or nation states that are far more powerful than any single person and making sure they for the most part benefit humanity. And also technology develops gradually. The so-called hot takeoff scenario, where it's not really working today and suddenly one day overnight it works brilliantly and we achieve super intelligence in six over a world. That's just not realistic. And I think the AI technology would develop slowly, like all the technology and then it gives us plenty of time to make sure that we provide oversight and can manage it to be safe. And lastly, if you look at the real extinction of rest of humanity, such as fingers crossed an expendemic or climate change leading to a massive depopulation of some parts of the planet or much lower odds but maybe someday an asteroid doing to us what it had done to the dinosaurs. I think we look at the actual growing extinction risk humanity AI having more intelligence, even artificial intelligence in the world, will be a key part of the solution. So I feel like if you want humanity to survive and thrive for the next thousand years rather than slowing AI down which some people propose I would rather make AI go as fast as possible. Some of that just summarizes my last slide. I think that AI as a general purpose technology creates a lot of new opportunities for everyone and a lot of the exciting and important work that lies ahead of us all is to go and build those concrete use cases and hopefully in the future, hopefully I have opportunities to maybe engage with more of you on those opportunities as well. So that, let me just say thank you all very much. Thank you.\", metadata={'source': 'D:\\\\DataSquad_Tasks\\\\summarizer\\\\artifacts\\\\10_29_2023_01_57_36\\\\transcription\\\\transcript.txt'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fc075d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "output = map_reduce_chain(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fc9fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In this video, Dr. Andrew Wu discussed the similarities between AI and electricity, and how AI has changed millions of lives through both education and work. He explained that supervised learning and generative AI are the two most important tools for AI development and discussed how they can be used for a variety of tasks. He also discussed the progress of AI over the last decade, the tremendous momentum behind Genose Bay and other tools for supervised learning, and the opportunities for startups and large companies to create value through the use of general purpose technologies. Additionally, he discussed the difficulty of using AI outside of the consumer software internet, the inefficiencies of hiring many engineers to work on expensive projects, and the two trends for enabling custom AI systems to be built and deployed by pieces of factories. Finally, he discussed the process for building startups and the potential ethical issues of AI.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57ef9ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" It is my pleasure to welcome Dr. Andrew Wu tonight. Andrew is the Managing General Partner of AI Fund, founder of Deep Learning AI and lending AI, chairman and co-founder of Coursera, and an unjunct professor of computer science here at Stanford. Previously he had started and led the Google Brain team, which had helped Google adopt modern AI. And he was also director of the Stanford AI lab. About 8 million people, one in 1000 persons on the planet have taken an AI class from him and through both his education and his AI work. He has changed humor's lives. Please welcome Dr. An. Thank you Lisa. It's good to see everyone. So what I want to do today is chat new electricity. One of the difficult things to understand about AI is that it is a general purpose technology. Meaning that it's not useful only for one thing, but it's useful for lots of different applications. Kind of like electricity. If I were to ask you what is electricity good for? You know, it's not any one thing, there's a lot of things. So what do I like to do is start off sharing with you how I view the technology landscape and just to lead into the set of opportunities. So, a lot of hype, a lot of excitement about AI. And I think a good way to think about AI is as a collection of tools. So this includes a technique called supervised learning, which is very good at recognizing things or labeling things, and genitive AI, which is relatively new, exciting development. If you're familiar with AI, you may have heard of other tools, but I'm going to talk less about these additional tools. And I'll focus today on what I think are currently the two most important tools which are supervised learning and generative AI. So supervised learning is very good at labeling things or very good at computing input to outputs or A to B mappings given input A, give me an output B. For example, given an email, we can use supervised learning to label a spam or not spam. The most lucrative application of this that I've ever worked on is Pravee Online advertising where I give it an ad we can label the users likely to click on it and therefore show more relevant ads. For self-driving calls given the sense of readings of a car we can label it with where the other cars One project that my team at AFN worked on was shift route optimization. We're given a route to the ship is taking or considering taking. We can label that with how much fuel we think is to consume and use this chips more fuel efficient. The law work in automated visual inspection in factories. So you can take a picture of a smartphone that was just manufactured and labeled as a scratch or any other defect in it. Or if you want to build a restaurant review reputation monitoring system, you can have little piece of software that looks at online restaurant reviews and labels that as positive or negative sentiment. So one nice thing, one cool thing about supervised learning is that it's not useful for one thing. It's useful for all of these different applications and many more besides. Let me just walk through, you know, concretely, the workflow of one example of a supervised learning labeling things kind of project. If you want to build a system to label restaurant reviews, you then collect a few data points, so collect the data set where it say, you know, the best strategy sandwich great to say that this positive circles is slow. That's negative. My favorite should be curry is positive. And here I've shown three data points, but you're building this, you may get thousands of data points like this, or thousands of training examples, we call it. And the workflow of a machine learning project, when AI project is you get labeled data, maybe thousands of data points, then you have an AI entry team train an AI model to learn from this data. And then finally, you would find maybe a cloud service to run the trained I model and they can feed it, you know, less multi-either-vahad and less positive sentiment. And so I think the launch decade was maybe the decade of large scale supervised learning. What we found starting about 10, 15 years ago was if you were to train a small AI model, so train a small neural network with small deep learning algorithm, basically a small AI model, maybe not on a very powerful computer. Then as you've had more data, as performance, would get better for a little bit, but then it would flatten out, it would plateau, and it would stop being able to use the data to get better and better. But if you were to train a very large AI model, lots of compute on maybe powerful GPUs. Then as we scaled up the amount of data we gave the machine learning model, its performance would kind of keep on getting better and better. So this is why when I started and led the Google Brain team, the primary mission that I directed the team to solve at the time was, let's just build really, really large neural networks that we then fed a lot of data to and that recipe fortunately worked. And I think the idea of driving large compute and large scale data that recipes really helped us driven a lot of AI progress over the last decade. So if that was the last decade of AI, I think this decade is turning out to be also doing everything we had in supervised learning, but adding to it the exciting tool of Genese AI. So, many of you, maybe all of you, were played with charge-gbd and bar and so on. But just, you know, given a piece of text, which we call prompt. I love eating. If you run this multiple times, maybe you get bigger screen keys or my mother's me love or outer friends and the AI system can generate output like that. Given the amounts of buzz and excitement about genesis of AI, I thought I'd take just half a slide to see a little bit about how this works. So it turns out that, at least this type of text generation, the core of it is using supervised learning that inputs output mappings to repeatedly predict the next word. And so if your system reads on the internet to sentence like, my favorite food is a bagel with cream cheese and locks, then this is translated into a few data points where if it sees my favorite food is a in just case try to guess that the right next word was bagel or my favorite food is a bagel try to guess next word is worth and similarly you've a season So by taking text that you find on the internet or other sources and by using this input output supervised, to try to repeatedly predict the next word. If you train a very large AI system on hundreds of billions of words, or in the case of the largest model is now more than a trillion words, then you get a large language model like Chaggb. And, you know, there are additional other important technical details I talked about predicting the next word. Technically, these systems predict the next subword, a part of work called token, and then there are other techniques like RHSF for further tuning the air output to be more helpful on this and harmless. But at the heart of it is this using supervised learning to repeatedly predict the things where that that's really was enabling the exciting, you know, really fantastic progress on large language models. So, while many people have seen large language models as a fantastic consumer too, You can go to a website like check TV's website or bots or other launch language malls and users. I think it's fantastic too. There's one of the trends I think is still underappreciated, which is the power of large language models, not as it, not just as consumer to, but as it develop a to. So it turns out that there are applications that used to take me months to build that a lot of people can now build much faster by using a large language model. So specifically, the workflow for supervised learning, building the restaurant review system, say, would be that you need to get a bunch of labeled data, and maybe that takes a month to get a few thousand data points. And then have an AI team train and tune and really get optimized performance on your AI model, maybe that would take three months. Then find a cloud service to run it, make sure it's running robustly, make sure it's recognized, maybe that would take another three months. So pretty realistic timeline for building a commercial grade machine learning system is like six to 12 months. So teams I've led're often took roughly six or 12 months to build and deploy these systems and some of them turned out to be really valuable, but this is a realistic timeline for building and deploying a commercial-grade AI system. In contrast, with prompt base AI, where you write a prompt, this is what the workflow looks like. You can specify a prompt that takes maybe minutes or hours and then you can deploy it to the cloud and that takes maybe hours or days. So there are certain AI applications that used to take me literally six months, maybe a year to build that many teams around the world can now build in maybe a week. And I think this is already starting, but the best is still yet to come. This is starting to open up a flood of a lot more AI applications that can be built by a lot of people. So I think many people still underestimate the magnitude of the flood of customer applications that I think is going to come down the pipe. Now, I know you probably were not expecting me to write code in this presentation, but that's what I'm going to do. So it turns out this is all the code I need in order to write a sentence in classifier. So I'm gonna, you know, solve you with no Python, I guess, import some tools from OpenAI, and then I have this prompt that says classified detects the low, delimited by three dashes is having either a positive or negative sentiments. I don't know. to have a fantastic time Learned a lot and also made great new friends. So that's my problem. And now I'm just going to run it. And I've never run it before. So I really hope that thank goodness we got the right answer. And this is literally all the code it takes the build a sentiment classifier. And so today, you know, developers around the world can take literally maybe like 10 minutes to build a system like this. And that's a very exciting development. So one of the things I've been working on was trying to teach online classes about how to use prompt promoting, not just as a consumer, but as a developer too. So, just about the technology landscape. Let me now share my thoughts on what are some of the AI opportunities I see. This shows what I think is the value about three years from now. But the vast majority of financial value from AI today is I think supervised learning where for a single company like Google can be worth more than $100 billion a year. And also, there are millions of developers building supervised learning applications. So it's already massively valuable and also with tremendous momentum behind it just because of the sheer effort in finding applications and building applications. And in Genose Bay, I is the really exciting new entrance, which is much smaller right now. And then there are the other tools's I'm including for completeness we can you know If the size of these circles represent the value today, this is what I think it might grow to in three years. So supervised learning already really massive may double say in the next three years, from truly massive to even more massive. And James Sabei, which is much smaller today, I think we'll much more than double in the next three years because of the number of amounts of developer interest, the amount of venture capital investments and large corporate exploring applications. And I also just want to point out, three years is a very short time horizon. If it continues to compound at anything near this rate, then in six years,, will be even faster larger. But just light shaded region in green or orange, that light shaded region is where the opportunities for either new startups offer large companies and companies to create and create a value capture. technologies are general purpose technologies. So in the case of supervised learning, a lot of the work that had to be done over the last decade, but it's continuing for the next decade is to identify and to execute on the concrete use cases. And that process is also kicking off for Genes of AI. So for this part of the presentation, I hope you take away from it that general purpose technologies are useful for many different tasks. A lot of value remains to be created using supervised learning and even though we're nowhere near finishing figure out exciting use cases of supervised learning where there's other fantastic two of genus avi which further expands the set of things we can now do using a i But one caveat, which is that there will be short-term fads along the way. So I don't know if some of you might remember the app called LENZA. This is the app that will let you upload pictures of yourself and then render a cool picture of you as an astronaut or a scientist or something. And it was a good idea and people liked it. And it's rough, it's just so cold like crazy like that through last December. And then it did that. And that's because lensa was, it was a good idea, people liked it, but it was a relatively thin software layer on top of someone else's really powerful APIs. And so even though it was a useful product, It was in a defensive all-possess. And when I think about, you know, absolute lenser. I'm actually reminded that when Steve Jobs gave us the iPhone, shortly after someone wrote an app that I paid $1.199 for to turn on the LED, to turn the phone into flashlight. And that was also a good idea to write an app to turn on the LED light, but it was a defensible long turn. It also didn't create very long-term value because it was a easy replicated and underpriced and eventually incorporated into iOS. But with the rise of iOS, with the rise of iPhone, someone also figured out how to build things like Uber and Airbnb and Tinder. The very long term, very defensible businesses that created, you know, sustaining value. And I think with the rise of gender AI or the rise of new AI tools, I think what really excites me is the opportunity to create those really deep, really hard applications that hopefully can create very long-term value. So the first trend I want to share is as a general purpose technology and a lot of work that lies ahead of us is to find the very diverse use cases and to build them. This is a second trend I want to share with you. We'll release the YA AI isn't more widely adopted yet. It feels like a bunch of us have been talking about AI for like 15 years or something, but if you look at where the value of AI is today, a lot of it is still very concentrated and consumer-software internet. Once you go outside, you know, tech or consumer software internet, there's some air adoption but the law views very early. So why is that? It turns out if you were to take all current and potential AI projects and sought them in decreasing order value. Then to the left of this curve, the head of this curve are the multi billion dollar project the for e-commerce, product recommendations, or company Amazon. It turns out that about 10, 15 years ago, various of my friends and I, we figured out a recipe for how to hire, say, a hundred engineers to write one piece of software to serve more relevant ads and apply that one piece of software to billion users and generate massive financial value. So that works. But once you go outside, consume a software internet, how do anyone has 100 million or a billion users, they can write and apply one piece of software to you. So once you go to other industries as we go from the head of this curve on the left over to the long tail, these taking pictures of the piece they were making because they needed to do things like make sure that the cheese is spread evenly. So this is about a $5 million project, but that recipe of hiring 100 engineers or dozens of engineers to work on a five million dollar project that doesn't make sense. Or another example, working with an agriculture company that would then we figured out that we used cameras to find out how tall is the wheat. A wheat is often bento because of wind or rain or something. And we can chop off the wheat at the right height, then that results in more food for the farmer to sell, and it's also better for the environment. But this is another $5 million project that owed recipe of having a large group of high school engineers to work on this one project that doesn't make sense. And some of the materials grading, cloth grading, sheet metal grading, many project like this. So whereas to the left in the head of this curve there's a small number of let's say multi-billion dollar projects and we know how to execute those delivering value. In other industries, I'm seeing a very long tail of tens of thousands of, let's call them, five million dollar projects, then until now, it had been very difficult to excuse on because of the high cost of customization. The trend that I think is exciting is that the AI community has been building better tools that lets us aggregate these use cases and make it easy for the end user to do the customization. So specifically, I'm seeing a lot of exciting low code and no code tools that enable the user to customize the AI system. What this means is instead of me needing to worry that much about pictures of pizza. We have tools, we can start into C2s that can enable the IT department and the PISA making factory to train AI systems on their own pictures of PISA to realize this $5 million worth of value. And by the way, the pictures of Pizzer, they don't exist on the internet. So Google and Bing don't have access to these pictures. We need tools that can be used by really the piece of factory themselves to build and deploy and maintain their own custom AI system that works on their own pictures of PISA. And broadly, the technology for enabling this. Some of it is prompting. Text prompting, visual prompting, but really large language models and similar tools like that. Or a technology called data centcentric AI whereby instead of asking the piece of factory to write a lot of code, which is challenging. We can ask them to provide data, which turns out to be more feasible. And I think the second trend is important because I think this is a key part of the recipe for taking the value of AI, which so far still feels very concentrated in the tech world and consumer software into that world and pushing this out to all industries really to the rest of the economy, which sometimes is easy to forget. The rest of the economy is much bigger than the tech world. So, R2 trends are shared as a general purpose technology technology lots of compute use cases to be realized as well as local no code easily used tools enabling AI to be deployed in more industries. How do we go after these opportunities? So about five years ago, there was a puzzle I wanted to solve, which is I felt that many valuable AI projects are now possible. I was thinking, how do we get them done. And having led AI teams in Google and by do in big tech companies, I had a hard time figuring out how I could operate a team in a big tech company to go off. There are very diverse opportunities and everything from maritime shipping to education to financial services, the healthcare and all that. It's just very diverse use cases, very diverse, go to markets and very diverse, really, you know, customer bases and then applications. And I felt that the most efficient way to do this would be we can start a lot of different companies to pursue these very diverse opportunities. So that's why I end up starting AI fun, which is a venture studio that builds startups to pursue a diverse set of our opportunities. And of course, in addition to lots of startups in company companies also have a lot of opportunities to integrate AI into existing businesses. In fact, one pattern I'm seeing for incumbent businesses is distribution. distribution is often one of the cyclical advantages of incoming companies that they play the cards right can allow them to integrate So, I think of this as a, this is why I think of it as the AI stack. At the bottom level is the hardware semiconductor layer. Fantastic opportunities there but very capital intensive, very concentrated. So there's a lot of resources around the few winners. So some people can and should play there. I personally don't like to play there myself. There's also the infrastructure layer. Also fantastic opportunities, but very capital intensive, very concentrated, so I tend not to play that myself either. And then there's a developer tool later. What I showed you just now was I was actually using OpenAI's API as a developer tool. And then I think the developer tool sector is a hyper competitive. Look at all the startups chasing OpenAI right now, but there will be some mega winners. And so I sometimes play here, but primarily when I think of a meaningful technology advantage because I think that earns you the right or earns you a better shot at being one of the mega winners. And then lastly, even though a lot of the media attention in the buzz is in the infrastructure and developer tooling layer, it turns out that that layer can the application layer is even more successful. And we saw this with the rise of SAS as well. A lot of the buzz and excitement is on the technology, the tooling layer, which is fine, nothing wrong with that. But the only way for that to be successful is that the application layer is even more successful so that frankly they can generate enough revenue to pay the infrastructure and the tooling layer. So, let me mention one example. ArmorRy, it's actually just texting the CO yesterday. ArmorRy is a company that we built that uses AI for romantic relationship coaching. And just to point out, I'm an AI guy and I feel like I know nothing really about romance. And if you don't believe me, you can ask my wife if she will confirm that I know nothing about romance. But we want to build this. We want to get together with the former CEO of Tinder, a Renata Nibalb, and with my team's expertise in AI and her expertise in relationships. When she ran Tinder, she knows more about relationships. I think anyone I know, were able to build something pretty unique using AI for romantic relationship mentoring. The interesting thing about applications like these is when we look around, how many teams in the world are simultaneously expert in AI and in relationships. And so at the application layer, I'm seeing a lot of exciting opportunities that seem to the very large market, but where the competition set is very light relative to the magnitude of the opportunity. It's not that there are no competitors, but it's just much less intense compared to the developer tool of the infrastructure of the era. And so because I've spent a lot of time iterating on a process of building startups. What we're going to do is just very transparent to tell you the recipe we've developed for building startups. And so after many years of iteration and improvement, this is how we now build startups. My team's always had accesss a lot of different ideas, your internal generated ideas from partners, and I want to walk through this with one example or something we did, which is a company bearing AI, which uses AI to make ships more fuel efficient. So this idea came to me when a few years ago a large Japanese conglomerate calledui, that is a major shareholder in the opera's major shipping lines. They came to me and they said, hey, Andrew, you should build a business to use AI to make ships more fuel efficient. And the specific idea was, think of it as a Google Maps for ships. We can suggest a ship or tell a ship how to steer so that you still get your destination on time by using turns out about 10% less fuel. And so what we now do is we spend about a month validating the idea. So double check this is idea even technically feasible and in terms of prospective customers to make sure that this is a market need. So we spent up to about a month doing that. And if it passes this stage, then we will go and recruit a CEO to work with us on the project. When I was starting out, I used to spend a long time working on the project myself before bringing on the CEO, but after iterating, we realized that bringing on the leader at the very beginning to work with us, it reduces a lot of the burden of having to transfer knowledge or having a CO come in and have to revalidate whether we discover it. So the process is we've learned much more efficient which is bringing the leader at the very start. And so in the case of bearing AI, we found a fantastic CEO, Dylan Kyle, who's repeat entrepreneur one successful exit before and then we spent three months six two weeks sprints to work with them to go the prototype as well as do do deep customer validation. If it survives the stage and we have about a 2.36% survival rate, we never write a first check in, which then gives the company resources to hire an executive team, you know, build the key team, get the MVP working, minima viable product working, and get some real customers. And then after that, hopefully then successfully raises additional external rounds of funding that can keep on growing and scaling. So I'm really proud of the work that my team was able to do to support Mitsui's idea and Dylan Kau as CEO. And today there are hundreds of ships on the high seas right now that are steering themselves differently because of Bering AI and 10% fuel savings translates to rough water amounts to maybe $450,000 in savings and fuel per year. And of course, it's also, frankly, quite a bit better for the environment. And I think this is not a, I think would not have existed if not for the lens fantastic work. And then also, you know, Mitzi, praying this idea to me. And I like this example because this is another one. It's like, you know, this is a starting idea that I would never have come up with myself, because I've been on a boat, but what do I know about maritime shipping? But is the deep, suction-matte expertise of Mitsui that had to zunzai together with Dylan and then my team's expertise in AI that made this possible. And so as I operate an AI, one thing I've learned is my swim lane is AI, that's it because I don't have time. It's very difficult for me to be expert in maritime shipping and romantic relationships and healthcare and financial services and all and all and all. And so I've learned that if I can just help get accurate technical validation and then use AR resources to make sure the AI tech has been quickly and well. And I think we've always managed to help the companies build a strong tech and protein, then partnering with subject matter experts often results in the collecting opportunities. And I want to share with you one other weird aspect of one of the weird aspect of, and one other weird lesson of learning about building startups, which is I like to engage only when there's a concrete idea. And this runs counter to bother the advice you hear from the design thinking methodology which often says don't rush to solutioning right explore a lot of alternatives and avoid a solution, we tried that. It was very slow. But what we've learned is that at the ideation stage, if someone comes to me and says, hey, Andrew, you should apply your IIT to financial services. Because I'm not a subject matter expert in financial services is very slow for me to learn enough about financial services to even figure out what to do. I mean, eventually you could get a good outcome, but it's a very labor intensive, very slow, very expensive process with me to try to learn industry after industry. In contrast, one of my partners wrote his ideas at Tony Che cheap, not really seriously. But let's say the congares is by GBT, let's eliminate commercials by automatically buying every product advertised in exchange for not having seen ads. It's not a good idea, but it is a concrete idea. And it turns out concrete ideas can be validated or falsified efficiently. They also give a team a clear direction to execute. And I've learned it in today's world, especially with the excitement and buzz and exposure to the AI of a lot of people, it turns out that there are a lot of subject matter experts in today's world that have deeply thought about the problem for months, sometimes even one or two years, but they've not yet had a build upon there and when we get together with them and Here and they share the idea of us it allows us to work with them to very quickly go into validation and building. And I find that this works because there are a lot of people that have already done the design thinking thing of exploring a lot of ideas and winning down to really good ideas. And there are, I find that there's so many good ideas sitting out there that no one is working on that finding those good ideas that someone has already had and wants to share with us and wants to build part of the floor. That turns out to be much more efficient engine. So, before I wrap up, we'll go to the question a second. Just a few slides to talk about risk and social impact. So, it's very powerful technology to state something you pray, my team and I work on projects and move humanity forward. We have multiple times, code projects that we assess we financially sound based on ethical grounds. It turns out I've been surprised and sometimes this made a creativity of people to come up with good ideas, so to come up with really bad ideas that seem profitable but really should not be built with a few projects on those grounds. And then I think it has to be acknowledged that AI today does have problems with bias, fairness, accuracy, but also, you know, technology is improving quickly. So I see that AI systems today are less buyers than six months ago and more fair than six months ago, which is not to dismiss the importance of these problems. They are problems that we should continue to work on them. But I'm also gratified at the number of AITs working hot on these issues to make them much better. When I think of the biggest risk of AI, I think that the biggest risk, one of the biggest risk is the disruption to jobs. This is a diagram from a paper by our friend at the University of Pennsylvania and some folks at OpenAI, analyzing the exposure of different jobs to AI automation. And it turns out that whereas the previous wave of automation mainly the most The most exposed jobs are often the lower wage jobs, such as when we put robots into factories with this current wave of automation is actually the higher-wage shops further the right of this axis that seems to So even as we create tremendous value using AI, I feel like as citizens and corporations and the governments and really our society, I feel a strong obligation to make sure that people, especially people who are lively, who are disrupted, are still well taken care of, are still treated well. And then lastly, there's also been, it feels like every time there's a big wave of progress in AI, there's a big wave of hype about artificial gender intelligence as well. When deep learning starts to work really well 10 years ago, there was a lot of hype about AGI and now the gen of AGI is working really well. There's another wave of hyperbubble AGI. But I don't think there's any time soon. One of the challenges is that the biological path to intelligence, like humans and the paths intelligence, you know AI, they've taken very different paths and the funny thing about the definition of AGI is, benchmarking, there's very different digital parts intelligence with really the biological parts intelligence. So I think, you know, Larsson Kraschmoldo is as smarter than any of us in certain key dimensions, but much dumber than any of us in other dimensions. And so forcing it to do everything a human can do is like a funny comparison. But I hope we'll get there, maybe hopefully within a lot of times. And then there's also a lot of, I think think overblown hype about AI creating extinction risk for humanity. Candidly, I don't see it. I just don't see how AI trees, any meaningful extinction risk of humanity. I think that people worry we can't control AI but we have lots of AI will be more powerful than any person but with lots of experience steering very powerful entities such as corporations or nation states that are far more powerful than any single person and making sure they for the most part benefit humanity. And also technology develops gradually. The so-called hot takeoff scenario, where it's not really working today and suddenly one day overnight it works brilliantly and we achieve super intelligence in six over a world. That's just not realistic. And I think the AI technology would develop slowly, like all the technology and then it gives us plenty of time to make sure that we provide oversight and can manage it to be safe. And lastly, if you look at the real extinction of rest of humanity, such as fingers crossed an expendemic or climate change leading to a massive depopulation of some parts of the planet or much lower odds but maybe someday an asteroid doing to us what it had done to the dinosaurs. I think we look at the actual growing extinction risk humanity AI having more intelligence, even artificial intelligence in the world, will be a key part of the solution. So I feel like if you want humanity to survive and thrive for the next thousand years rather than slowing AI down which some people propose I would rather make AI go as fast as possible. Some of that just summarizes my last slide. I think that AI as a general purpose technology creates a lot of new opportunities for everyone and a lot of the exciting and important work that lies ahead of us all is to go and build those concrete use cases and hopefully in the future, hopefully I have opportunities to maybe engage with more of you on those opportunities as well. So that, let me just say thank you all very much. Thank you.\", metadata={'source': 'D:\\\\DataSquad_Tasks\\\\summarizer\\\\artifacts\\\\10_29_2023_01_57_36\\\\transcription\\\\transcript.txt'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c67fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcript docs [Document(page_content='Now here are 5 major reasons to wait for Galaxy S24 Ultra. The S24 Ultra is expected to be an AI powerhouse offering features from chatGPD and Google Bud \\nsuch as the ability to create contents based on keywords you input. The text to image generative AI is also expected to be on S24 Ultra. Though the phone is expected to look simila\\nr to the S23 Ultra, this time the phone is going to feature a titanium frame which is going to be more durable. The pre-release benchmark scores of Snapdragon 8 Gen 3 or the Xenos \\n2400 appears to be great and we can expect a custom the The RAMs and is expected to drop the 10x optical zoom which may seem like a downgrade but that is expected to be replaced by\\n 5x optical zoom which is going to be more practical and useful for sure. The main camera may come with the same tone megapixels but the 5x telephoto lens is expected to be 50 mega', metadata={}), Document(page_content='pixels which is going to be great and the night photography and video photography is expected to be much better compared to the predecessors. Now are you planning to upgrade to S24 Ultra? Drop a comment, share your thoughts and subscribe to this channel for more tips.', metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Now here are 5 major reasons to wait for Galaxy S24 Ultra. The S24 Ultra is expected to be an AI powerhouse offering features from chatGPD and Google Bud \\nsuch as the ability to create contents based on keywords you input. The text to image generative AI is also expected to be on S24 Ultra. Though the phone is expected to look simila\\nr to the S23 Ultra, this time the phone is going to feature a titanium frame which is going to be more durable. The pre-release benchmark scores of Snapdragon 8 Gen 3 or the Xenos \\n2400 appears to be great and we can expect a custom the The RAMs and is expected to drop the 10x optical zoom which may seem like a downgrade but that is expected to be replaced by\\n 5x optical zoom which is going to be more practical and useful for sure. The main camera may come with the same tone megapixels but the 5x telephoto lens is expected to be 50 mega', metadata={}),\n",
       "  Document(page_content='pixels which is going to be great and the night photography and video photography is expected to be much better compared to the predecessors. Now are you planning to upgrade to S24 Ultra? Drop a comment, share your thoughts and subscribe to this channel for more tips.', metadata={})],\n",
       " 'output_text': ' The Samsung S24 Ultra is an AI powerhouse expected to feature a titanium frame, Snapdragon 8 Gen 3 or Xenos 2400 processor, and a 50 megapixel 5x telephoto lens. It will also offer features such as text to image generative AI and Google Bud, replacing the 10x optical zoom with a more practical and useful 5x optical zoom. The speaker encourages viewers to upgrade to this phone, share their thoughts in the comments, and subscribe for more tips.'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.pipeline.summarizer_pipeline import SummarizerPipeline\n",
    "\n",
    "txt = \"\"\"Now here are 5 major reasons to wait for Galaxy S24 Ultra. The S24 Ultra is expected to be an AI powerhouse offering features from chatGPD and Google Bud \n",
    "such as the ability to create contents based on keywords you input. The text to image generative AI is also expected to be on S24 Ultra. Though the phone is expected to look simila\n",
    "r to the S23 Ultra, this time the phone is going to feature a titanium frame which is going to be more durable. The pre-release benchmark scores of Snapdragon 8 Gen 3 or the Xenos \n",
    "2400 appears to be great and we can expect a custom the The RAMs and is expected to drop the 10x optical zoom which may seem like a downgrade but that is expected to be replaced by\n",
    " 5x optical zoom which is going to be more practical and useful for sure. The main camera may come with the same tone megapixels but the 5x telephoto lens is expected to be 50 mega\n",
    "pixels which is going to be great and the night photography and video photography is expected to be much better compared to the predecessors. Now are you planning to upgrade to S24 Ultra? Drop a comment, share your thoughts and subscribe to this channel for more tips.\n",
    "\"\"\"\n",
    "SummarizerPipeline().summarize_transcript(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
